{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3\n",
        "\n",
        "\n",
        "\n",
        "*   Akshay Kumar\n",
        "*   CS23MTECH11022\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fsp-3L_-dlpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "o-iGaW5bdKSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Self-Attention for Object Recognition with CNNs: Implement a sample CNN with one or\n",
        "more self-attention layer(s) for performing object recognition over CIFAR-10 dataset. You have to\n",
        "implement the self-attention layer yourself and use it in the forward function defined by you. All\n",
        "other layers (fully connected, nonlinearity, conv layer, etc.) can be bulit-in implementations. The\n",
        "network can be a simpler one (e.g., it may have 1x Conv, 4x [Conv followed by SA], 1x Conv, and\n",
        "1x GAP). Please refer to the reading material provided here or any other similar one."
      ],
      "metadata": {
        "id": "oncDIIsadTqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries to load the CIFAR-10 dataset.\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Convert it into a PyTorch tensor, and normalize it for each channel.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Loading CIFAR-10 Dataset.\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# PyTorch utility that helps with efficient data loading.\n",
        "training_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "testing_data = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBxi80u9fE0j",
        "outputId": "1db1bc6b-9f5d-4e6d-a227-01797e018a91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13168225.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y04m-jvKRDsJ",
        "outputId": "7e569376-a867-46cc-c6a6-a8f0e796cb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.2039, Epoch [1]\n",
            "Loss: 1.7772, Epoch [2]\n",
            "Loss: 1.2344, Epoch [3]\n",
            "Loss: 1.0336, Epoch [4]\n",
            "Loss: 0.9048, Epoch [5]\n",
            "Loss: 0.8108, Epoch [6]\n",
            "Loss: 0.7366, Epoch [7]\n",
            "Loss: 0.6769, Epoch [8]\n",
            "Loss: 0.6208, Epoch [9]\n",
            "Loss: 0.5793, Epoch [10]\n",
            "Accuracy on test dataset of CIFAR 10 dataset: 73.25%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Defining the Self-Attention Layer\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dimension):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        # Query, Key, and Value convolutions for self-attention mechanism.\n",
        "        self.query_convolution = nn.Conv2d(in_channels=input_dimension, out_channels=input_dimension//8, kernel_size=1)\n",
        "        self.key_convolution = nn.Conv2d(in_channels=input_dimension, out_channels=input_dimension//8, kernel_size=1)\n",
        "        self.value_convolution = nn.Conv2d(in_channels=input_dimension, out_channels=input_dimension, kernel_size=1)\n",
        "\n",
        "        # Initializing learnable parameter for scaling the attention output.\n",
        "        self.gamma_parameter = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    # Forward pass.\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "\n",
        "        # Query, Key, and Value projections.\n",
        "        projected_query = self.query_convolution(x).view(batch_size, -1, width*height).permute(0, 2, 1)\n",
        "        projected_key = self.key_convolution(x).view(batch_size, -1, width*height)\n",
        "        energy = torch.bmm(projected_query, projected_key)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        projected_value = self.value_convolution(x).view(batch_size, -1, width*height)\n",
        "\n",
        "        # Calculating output of self-attention.\n",
        "        output = torch.bmm(projected_value, attention.permute(0, 2, 1))\n",
        "        output = output.view(batch_size, channels, height, width)\n",
        "        output = self.gamma_parameter * output + x\n",
        "        return output\n",
        "\n",
        "# Defining CNN Architecture with Self-Attention.\n",
        "class CNNWithSelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithSelfAttention, self).__init__()\n",
        "\n",
        "        # Convolutional layers and self-attention modules.\n",
        "        self.convolution1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.sa1 = SelfAttentionLayer(32)\n",
        "        self.convolution2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.sa2 = SelfAttentionLayer(64)\n",
        "        self.convolution3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.sa3 = SelfAttentionLayer(128)\n",
        "        self.convolution4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.sa4 = SelfAttentionLayer(256)\n",
        "        self.convolution5 = nn.Conv2d(256, 10, 3, padding=1)\n",
        "        self.global_average_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Forward pass through the network\n",
        "        x = F.relu(self.convolution1(x))\n",
        "        x = self.sa1(x)\n",
        "        x = F.relu(self.convolution2(x))\n",
        "        x = self.sa2(x)\n",
        "        x = F.relu(self.convolution3(x))\n",
        "        x = self.sa3(x)\n",
        "        x = F.relu(self.convolution4(x))\n",
        "        x = self.sa4(x)\n",
        "        x = F.relu(self.convolution5(x))\n",
        "        x = self.global_average_pooling(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "# Instantiating the model.\n",
        "model = CNNWithSelfAttention()\n",
        "\n",
        "# Defining loss function and optimizer.\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterating over the training dataset.\n",
        "    for input_data, labels_data in training_data:\n",
        "        input_data, labels_data = input_data.to(device), labels_data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_data)\n",
        "        loss = loss_function(outputs, labels_data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * input_data.size(0)\n",
        "\n",
        "    # Calculating loss in each epoch and printing it.\n",
        "    epoch_loss = running_loss / len(training_data.dataset)\n",
        "    print(f\"Loss: {epoch_loss:.4f}, Epoch [{epoch+1}]\")\n",
        "\n",
        "# Testing loop\n",
        "model.eval()\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Iterating over the testing dataset\n",
        "    for input_data, labels_data in testing_data:\n",
        "        input_data, labels_data = input_data.to(device), labels_data.to(device)\n",
        "        outputs = model(input_data)\n",
        "        _, predicted_labels = torch.max(outputs, 1)\n",
        "        total_predictions += labels_data.size(0)\n",
        "        correct_predictions += (predicted_labels == labels_data).sum().item()\n",
        "\n",
        "# Calculating accuracy and printing it.\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Accuracy on test dataset of CIFAR 10 dataset: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of parameters\n",
        "total_params_cnn_sa = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters in CNN with Self-Attention model:\", total_params_cnn_sa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2aFj5bqdyyP",
        "outputId": "df358b25-cbdb-45ac-f12c-05cc1ad9d8a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters in CNN with Self-Attention model: 520870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "A_X72fdZdOtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Object Recognition with Vision Transformer: Implement and train an Encoder only Trans-\n",
        "former (ViT-like) for the above object recognition task. In other words, implement multi-headed self-attention for the image classification (i.e., appending a < class > token to the image patches\n",
        "that are accepted as input tokens). Compare the performance of the two implementations (try to\n",
        "keep the number of parameters to be comparable and use the same amount of training and testing\n",
        "data)."
      ],
      "metadata": {
        "id": "jgaIDlMEdc_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Multi-Head Self-Attention Module.\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert input_dim % num_heads == 0, \"Input dimension must be divisible by the number of heads.\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = input_dim // num_heads\n",
        "\n",
        "        # Applying linear transformations for queries, keys, and values.\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        # Final linear transformation\n",
        "        self.fc_out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Applying linear transformation and splitting into multiple heads.\n",
        "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Finding ranspose for batch matrix multiplication.\n",
        "        Q = Q.permute(0, 2, 1, 3)\n",
        "        K = K.permute(0, 2, 1, 3)\n",
        "        V = V.permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculating attention scores\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.head_dim\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "        # Finding weighted sum of values\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, seq_len, self.input_dim)\n",
        "\n",
        "        # Final linear transformation\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "# Defining Transformer Encoder Block\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_att = self.dropout(self.self_attention(x))\n",
        "        # finding layer normalization and feed forward.\n",
        "        x = self.norm1(x + x_att)\n",
        "        x_ff = self.dropout(self.feed_forward(x))\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        x = self.norm2(x + x_ff)\n",
        "        return x\n",
        "\n",
        "# Define Vision Transformer\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, embed_dim, num_heads, mlp_dim, num_layers, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding and positional embedding\n",
        "        self.patch_embedding = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer encoder blocks\n",
        "        self.transformer_encoder_blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Class token\n",
        "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extracting the dimensions of the input tensor x.\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Patch embedding: applying a convolution operation to extract image patches\n",
        "        #and embedding them into a lower-dimensional space.\n",
        "        x = self.patch_embedding(x).flatten(2).transpose(1, 2)  # (B, embed_dim, num_patches)\n",
        "\n",
        "        # Adding a class token to the embedded patches.\n",
        "        class_token = self.class_token.expand(B, -1, -1)\n",
        "        x = torch.cat([class_token, x], dim=1)\n",
        "\n",
        "        # Adding positional embeddings to the embedded patches.\n",
        "        x += self.positional_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Iterate through transformer encoder blocks\n",
        "        for transformer_encoder_block in self.transformer_encoder_blocks:\n",
        "            x = transformer_encoder_block(x)\n",
        "\n",
        "        # Extracting class token\n",
        "        x = x[:, 0]\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "# Initalizing the ViT model.\n",
        "image_size = 32\n",
        "patch_size = 16\n",
        "num_classes = 10\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "mlp_dim = 256\n",
        "num_layers = 3\n",
        "dropout = 0.1\n",
        "\n",
        "model = VisionTransformer(image_size, patch_size, num_classes, embed_dim, num_heads, mlp_dim, num_layers, dropout)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterating over the training dataset.\n",
        "    for input_data, labels_data in training_data:\n",
        "        input_data, labels_data = input_data.to(device), labels_data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_data)\n",
        "        loss = loss_function(outputs, labels_data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * input_data.size(0)\n",
        "\n",
        "    # Calculating loss in each epoch and printing it.\n",
        "    epoch_loss = running_loss / len(training_data.dataset)\n",
        "    print(f\"Loss: {epoch_loss:.4f}, Epoch [{epoch+1}]\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Testing loop\n",
        "with torch.no_grad():\n",
        "    # Iterating over the testing dataset\n",
        "    for input_data, labels_data in testing_data:\n",
        "        input_data, labels_data = input_data.to(device), labels_data.to(device)\n",
        "        outputs = model(input_data)\n",
        "        _, predicted_labels = torch.max(outputs, 1)\n",
        "        total_predictions += labels_data.size(0)\n",
        "        correct_predictions += (predicted_labels == labels_data).sum().item()\n",
        "\n",
        "# Calculating accuracy and printing it.\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Accuracy on test dataset of CIFAR 10 dataset: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaJxzngZdz5i",
        "outputId": "faa12f7c-d7bc-4053-df9e-59c8e00dae9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.7013, Epoch [1]\n",
            "Loss: 1.4873, Epoch [2]\n",
            "Loss: 1.3998, Epoch [3]\n",
            "Loss: 1.3399, Epoch [4]\n",
            "Loss: 1.2924, Epoch [5]\n",
            "Loss: 1.2506, Epoch [6]\n",
            "Loss: 1.2189, Epoch [7]\n",
            "Loss: 1.1862, Epoch [8]\n",
            "Loss: 1.1566, Epoch [9]\n",
            "Loss: 1.1307, Epoch [10]\n",
            "Accuracy on test dataset of CIFAR 10 dataset: 55.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy on test dataset of CIFAR 10 in Object Recognition with Vision Transformer is ~55% which is low as compared to Object Recognition with CNNs and Self-Attention where the accuracy on the same test dataset is ~73%.\n",
        "\n",
        "One of the reason for the same may be because of more number of convolution layers used in the CNN network."
      ],
      "metadata": {
        "id": "EUBc7Jmiv3in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of parameters\n",
        "total_params_vit = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters in ViT model:\", total_params_vit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_BrDBwzu7rW",
        "outputId": "7e595a15-e1b0-4029-8a2d-d5048d4b816c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters in ViT model: 497930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total number of parameters in the ViT model is 497930 which is  not exactly the same as in CNN with Self-Attention model which is 520870 parameters, but it is nearly comparable. When I tried to increase the number of parameters from this current scenario in ViT model then the accuracy drops suddenly."
      ],
      "metadata": {
        "id": "H0e0U2McQXkp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}